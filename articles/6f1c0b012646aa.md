---
title: "Ollama x Pythonで今更始めるローカルLLM"
emoji: "🦙"
type: "tech"
topics: ["ollama", "llm", "python"]
published: true
published_at: 2025-12-01 00:00
---

## はじめに

この記事は [あくあたん工房 Advent Calendar 2025](https://adventar.org/calendars/12284) の1日目の記事です．

https://adventar.org/calendars/12284

皆さん，LLM (大規模言語モデル)，使っていますか？
私は使っています．[ChatGPT](https://chatgpt.com/) とか，[Claude](https://claude.ai/)とか，[Gemini](https://gemini.google.com/) とか．
Webブラウザでチャットをしたり，コードエディタの拡張機能でコード補完をしたり，アプリケーションからAPI を叩いたり，使い方も様々です．
ただこれらを開発・研究用途で利用するとなると **レートリミット** や **コスト**， **プライバシ** など，様々な問題が出てくるため，ローカルで LLM を動かしたいという方も多いのではないでしょうか．

そこで本稿では今更ながら [Ollama](https://github.com/ollama/ollama) を用いてローカルで LLM を実行し，Python から利用する方法を解説します．
かなり今更な内容ですが，ローカル LLM に興味がある方の参考になれば幸いです．

## 環境構築

環境によってインストール手順が異なるため，以下では [mise](https://github.com/jdx/mise) を用いた手順を解説します．

mise は「開発環境のフロントエンド」を名乗るツールで，プログラミング言語やパッケージマネージャ，CLIツールなど，様々なツールのインストール・バージョン管理を一元化できます．

mise のインストール手順は次の通りです．

```sh
curl https://mise.run | sh
```

> 私はPython環境を [uv](https://github.com/astral-sh/uv) で管理しており，更に uv と Ollama を mise で管理しています．

### Ollama

Ollama は LLM を実行するための CLI ツールです．

類似のツールとして [LM Studio](https://lmstudio.ai/) が有名ですが，LM Studio は GUI アプリケーションであるのに対し，Ollama は CLI ツールであるという違いがあります．

Ollama のインストール手順は次の通りです．

```sh
mise use -g ollama
```

> `-g` はグローバルインストールの意で，これによりシステム全体で ollama が利用可能になります．

:::details mise を使わない場合 (Linux)

```sh
curl -fsSL https://ollama.com/install.sh | sh
```

より詳細な情報は公式ドキュメントを参照してください．

https://docs.ollama.com/linux

:::

:::details mise を使わない場合 (macOS)

公式ドキュメントでは dmg ファイルを用いたインストール方法が案内されていますが，Homebrew でもインストールできます．

```sh
brew install ollama
```

ollama-app というものもありますが，そちらは	GUI アプリケーションであり，本稿では扱いません．

:::

https://github.com/ollama/ollama

### uv (Python)

uv は Python のバージョン・パッケージ管理ツールです．

Python 環境をプロジェクトごとに分離できるため，Python 開発において非常に便利です．

uv のインストール手順は次の通りです．

```sh
mise use -g uv
```

> かつて Python の環境構築と言えば，プロジェクトごとに pyenv で Python のバージョン管理を，venv で仮想環境の構築を，pip でパッケージ管理をそれぞれ行うのが一般的でした．
> その後仮想環境の構築とパッケージ管理を行うツールとして Pipenv や Poetry など，様々なツールが登場しましたが，uv はこれら全ての機能を備えたツールで，今後の Python 環境構築のデファクトスタンダードに最も近いツールと言えます．

:::details mise を使わない場合 (Linux/macOS)

```sh
curl -LsSf https://astral.sh/uv/install.sh | sh
```

:::

## Ollama サーバの起動とモデルのダウンロード

次のコマンドで Ollama サーバを起動します．

```sh
ollama serve
```

このとき `Error: listen tcp x.x.x.x:11434: bind: address already in use` のようなエラーが出た場合，既に Ollama サーバが起動しているため，このエラーは無視して構いません．

次に，モデルをダウンロードします．
デフォルトで利用可能なモデルの一覧は次のドキュメントで確認できます．

https://ollama.com/search

ここでは例として `gemma3` モデルのうち，パラメータ数が `4B` (4billion) のモデルをダウンロードします．このときのコマンドは次の通りです．

```sh
ollama pull gemma3:4b
```

ここでモデル名は `<モデル名>:<パラメータ数>` の形式で指定します．

> 利用する計算機でモデルの性能を十分に発揮するには，モデルのパラメータ数，量子化ビット数に対して計算機の VRAM (GPU メモリ) 容量が十分である必要がありますが，ここでは詳細な説明は割愛します．

モデルのダウンロードが完了したら，次のコマンドでモデルが利用可能であることを確認します．

```sh
ollama list
```

実行結果一覧に `gemma3:4b` が含まれていればダウンロードできています．

このままモデルを実行できますが，本稿の目的は Python から LLM を利用することなので，ここでは割愛します．

:::details モデルの実行

`ollama run` コマンドを実行すると，CLI上でLLMと対話できます．

```sh
ollama run gemma3:4b
```

:::

## Python からの利用

Python から Ollama サーバにアクセスする手法は主に3つあります．

1. requests
    - Ollama サーバは REST API を提供しているため，HTTP リクエストでモデルを利用可能
2. openai
    - Ollama サーバは OpenAI API 互換の API を提供しているため，openai ライブラリを用いてモデルを利用可能
3. ollama
    - ollama ライブラリを用いてモデルを利用可能

それぞれにメリットがありますが，ここでは最も簡潔に記述できる ollama ライブラリを用いた手法を紹介します．

### Python プロジェクトの作成

まず uv を用いて Python プロジェクトを作成します．

```sh
uv init ollama_example
```

ここではカレントディレクトリ直下に `ollama_example` という名前のプロジェクトを作成しています．

次にプロジェクトディレクトリに移動します．

```shsh
cd ollama_example
```

### ollama ライブラリのインストール

次のコマンドで `ollama` ライブラリをインストールします．

```sh
uv add ollama
```

> uv では `uv add` コマンドでパッケージをインストールすると，自動的に仮想環境が作成され，その仮想環境にパッケージがインストールされます．

### サンプルプログラム

`ollama`ライブラリを用いて `gemma3:4b` モデルを呼び出して対話するサンプルプログラムは次の通りです．
uv を用いてプロジェクトを作成した場合，デフォルトで `main.py` という名前のファイルが作成されるため，このファイルを編集します．

```py: main.py
import ollama


def main():
    response = ollama.chat(
        model="gemma3:4b",
        messages=[
            {
                "role": "user",
                "content": "Hello, world!",
            }
        ],
    )

    print(response["message"]["content"])


if __name__ == "__main__":
    main()
```

非常に簡潔です．`ollama.chat()` 関数にモデル名とメッセージを渡すだけで，モデルからの応答を取得できます．

このプログラムを次のコマンドで実行します．

```sh
uv run main.py
```

すると実行結果としてモデルからの応答が表示されます．

## まとめ

本稿ではOllama サーバを起動し，モデルをダウンロードして Python から利用する方法を解説しました．
とはいえこれだけではただのチャットボットと何も変わりません．
時間がないので書ききれませんでしたが，アドベントカレンダー期間中により実践的な利用方法の紹介を書ければと思っています．

それでは．
